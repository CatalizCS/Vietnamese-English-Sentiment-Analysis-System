{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Model Testing and Evaluation\n",
       "\n",
       "This notebook demonstrates testing and evaluating sentiment analysis models for both Vietnamese and English."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import sys\n",
       "import os\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import joblib\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from sklearn.metrics import confusion_matrix, classification_report\n",
       "\n",
       "# Add project root to path\n",
       "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
       "sys.path.append(project_root)\n",
       "\n",
       "from src.config import Config\n",
       "from src.models.model_predictor import SentimentPredictor\n",
       "from src.data.preprocessor import DataPreprocessor\n",
       "from src.features.feature_engineering import FeatureExtractor"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def check_model_exists(language, config):\n",
       "    \"\"\"Check if trained model exists for given language\"\"\"\n",
       "    model_path = os.path.join(config.DATA_DIR, \"models\", f\"{language}_sentiment_model.pkl\")\n",
       "    return os.path.exists(model_path)\n",
       "\n",
       "def load_model_components(language):\n",
       "    \"\"\"Load model components if model exists\"\"\"\n",
       "    config = Config()\n",
       "    \n",
       "    if not check_model_exists(language, config):\n",
       "        print(f\"No trained model found for {language}\")\n",
       "        return None, None, None\n",
       "        \n",
       "    try:\n",
       "        predictor = SentimentPredictor(language, config)\n",
       "        preprocessor = DataPreprocessor(language, config)\n",
       "        feature_extractor = FeatureExtractor(language, config)\n",
       "        print(f\"Successfully loaded model components for {language}\")\n",
       "        return predictor, preprocessor, feature_extractor\n",
       "    except Exception as e:\n",
       "        print(f\"Error loading model for {language}: {str(e)}\")\n",
       "        return None, None, None\n",
       "\n",
       "# Load available models\n",
       "available_models = {}\n",
       "for lang in ['vi', 'en']:\n",
       "    components = load_model_components(lang)\n",
       "    if all(components):\n",
       "        available_models[lang] = {\n",
       "            'predictor': components[0],\n",
       "            'preprocessor': components[1],\n",
       "            'feature_extractor': components[2]\n",
       "        }\n",
       "\n",
       "if not available_models:\n",
       "    print(\"No models available for testing. Please train models first.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Test Individual Samples"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def predict_sentiment(text, language='vi'):\n",
       "    \"\"\"Predict sentiment for a single text\"\"\"\n",
       "    if language not in available_models:\n",
       "        print(f\"No model available for {language}\")\n",
       "        return None\n",
       "    \n",
       "    components = available_models[language]\n",
       "    \n",
       "    # Create DataFrame with single text\n",
       "    df = pd.DataFrame({'text': [text]})\n",
       "    \n",
       "    # Process and predict\n",
       "    try:\n",
       "        processed_df = components['preprocessor'].preprocess(df)\n",
       "        features = components['feature_extractor'].extract_features(processed_df['cleaned_text'])\n",
       "        prediction = components['predictor'].predict(features)[0]\n",
       "        probabilities = components['predictor'].predict_proba(features)[0]\n",
       "        detailed_emotions = components['predictor'].predict_detailed_emotion(features)[0]\n",
       "        \n",
       "        sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
       "        \n",
       "        print(f\"Text: {text}\")\n",
       "        print(f\"Sentiment: {sentiment_map[prediction]} (confidence: {max(probabilities):.2f})\")\n",
       "        print(f\"Detailed emotion: {detailed_emotions['detailed_emotion']}\")\n",
       "        return prediction, probabilities, detailed_emotions\n",
       "    except Exception as e:\n",
       "        print(f\"Error processing text: {str(e)}\")\n",
       "        return None\n",
       "\n",
       "# Test samples for available languages\n",
       "test_samples = {\n",
       "    'vi': [\n",
       "        \"Sản phẩm tuyệt vời, rất đáng tiền\",\n",
       "        \"Dịch vụ quá tệ, không bao giờ quay lại\",\n",
       "        \"Tạm được, không tốt không xấu\"\n",
       "    ],\n",
       "    'en': [\n",
       "        \"This product is amazing, totally worth it!\",\n",
       "        \"Terrible service, never coming back\",\n",
       "        \"It's okay, nothing special\"\n",
       "    ]\n",
       "}\n",
       "\n",
       "for lang in available_models:\n",
       "    print(f\"\\nTesting {lang.upper()} model:\")\n",
       "    print(\"-\" * 50)\n",
       "    for sample in test_samples[lang]:\n",
       "        predict_sentiment(sample, lang)\n",
       "        print(\"-\" * 30)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Batch Testing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def evaluate_model(test_data, language='vi'):\n",
       "    \"\"\"Evaluate model on test dataset\"\"\"\n",
       "    if language not in available_models:\n",
       "        print(f\"No model available for {language}\")\n",
       "        return None\n",
       "    \n",
       "    components = available_models[language]\n",
       "    \n",
       "    try:\n",
       "        # Preprocess test data\n",
       "        processed_df = components['preprocessor'].preprocess(test_data)\n",
       "        features = components['feature_extractor'].extract_features(processed_df['cleaned_text'])\n",
       "        \n",
       "        # Get predictions\n",
       "        predictions = components['predictor'].predict(features)\n",
       "        probabilities = components['predictor'].predict_proba(features)\n",
       "        \n",
       "        # Calculate metrics\n",
       "        conf_matrix = confusion_matrix(processed_df['label'], predictions)\n",
       "        class_report = classification_report(processed_df['label'], predictions)\n",
       "        \n",
       "        # Plot confusion matrix\n",
       "        plt.figure(figsize=(8, 6))\n",
       "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
       "        plt.title(f'Confusion Matrix - {language.upper()}')\n",
       "        plt.ylabel('True Label')\n",
       "        plt.xlabel('Predicted Label')\n",
       "        plt.show()\n",
       "        \n",
       "        print(\"\\nClassification Report:\")\n",
       "        print(class_report)\n",
       "        \n",
       "        return predictions, probabilities, conf_matrix, class_report\n",
       "    except Exception as e:\n",
       "        print(f\"Error evaluating model: {str(e)}\")\n",
       "        return None\n",
       "\n",
       "# Evaluate available models\n",
       "for lang in available_models:\n",
       "    print(f\"\\nEvaluating {lang.upper()} model:\")\n",
       "    print(\"-\" * 50)\n",
       "    \n",
       "    try:\n",
       "        test_data = pd.read_csv(os.path.join(project_root, 'data', 'processed', f'{lang}_processed_data.csv'))\n",
       "        evaluate_model(test_data, lang)\n",
       "    except Exception as e:\n",
       "        print(f\"Error loading test data for {lang}: {str(e)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Error Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def analyze_errors(test_data, predictions, probabilities, language='vi'):\n",
       "    df = test_data.copy()\n",
       "    df['predicted'] = predictions\n",
       "    df['confidence'] = np.max(probabilities, axis=1)\n",
       "    \n",
       "    # Find misclassified samples\n",
       "    errors = df[df['label'] != df['predicted']].copy()\n",
       "    errors['confidence'] = errors['confidence'].round(3)\n",
       "    \n",
       "    print(f\"Total errors: {len(errors)}\")\n",
       "    print(\"\\nSample errors with highest confidence:\")\n",
       "    print(errors.sort_values('confidence', ascending=False)[['text', 'label', 'predicted', 'confidence']].head())\n",
       "    \n",
       "    # Plot confidence distribution\n",
       "    plt.figure(figsize=(10, 5))\n",
       "    plt.hist(errors['confidence'], bins=20)\n",
       "    plt.title('Confidence Distribution of Errors')\n",
       "    plt.xlabel('Confidence')\n",
       "    plt.ylabel('Count')\n",
       "    plt.show()\n",
       "    \n",
       "    return errors\n",
       "\n",
       "# Analyze errors for both languages\n",
       "print(\"Vietnamese Error Analysis:\")\n",
       "vi_errors = analyze_errors(vi_test_data, vi_results[0], vi_results[1], 'vi')\n",
       "\n",
       "print(\"\\nEnglish Error Analysis:\")\n",
       "en_errors = analyze_errors(en_test_data, en_results[0], en_results[1], 'en')"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }